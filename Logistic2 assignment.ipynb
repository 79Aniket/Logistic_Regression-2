{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b2f0d09-836d-4ec6-9361-92dabef0c99a",
   "metadata": {},
   "source": [
    "## Q1. What is the purpose of grid search cv in machine learning, and how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11199bdc-a1c9-4979-9abd-c524ed43f16d",
   "metadata": {},
   "source": [
    "## Grid search CV is a technique used in machine learning to find the optimal hyperparameters for a given model. Hyperparameters are the parameters that control the learning process of the model, and they are not learned from the data. The optimal hyperparameters are the ones that produce the best model performance on the unseen data.\n",
    "## Grid search CV works by exhaustively searching over a grid of hyperparameter values. The grid is a multidimensional table, where each row represents a different combination of hyperparameter values. Grid search CV then fits the model for each combination of hyperparameter values and evaluates its performance on a holdout dataset. The combination of hyperparameter values that produces the best model performance is selected as the optimal hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377e3a01-d5b4-4105-a0ca-0730ceb951d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb6635dc-30f3-453d-aa09-05f25d1eafa1",
   "metadata": {},
   "source": [
    "## Q2. Describe the difference between grid search cv and randomize search cv, and when might you choose one over the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f4682f-9f6a-4d4b-8dcd-c08bf26f730d",
   "metadata": {},
   "source": [
    "## Grid search CV exhaustively searches over a grid of hyperparameter values. This means that it tries all possible combinations of hyperparameter values, which can be computationally expensive, especially if the grid of hyperparameter values is large. However, grid search CV is a more thorough approach to hyperparameter tuning, and it is less likely to miss the optimal hyperparameters.\n",
    "\n",
    "## Randomized search CV randomly samples hyperparameter values from a distribution. This is less computationally expensive than grid search CV, but it is also less thorough. However, random search CV can be a good choice if you are not sure which hyperparameters to search over, or if you have a limited amount of time or resources.\n",
    "## The best way to choose between grid search CV and random search CV is to experiment with both methods and see which one works better for your specific problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b1fcb2-8bc3-4e02-ad40-08801600d079",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50f84c36-cf1f-4316-803d-ac3c9ae99c73",
   "metadata": {},
   "source": [
    "## Q3. What is data leakage, and why is it a problem in machine learning? Provide an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16236689-1822-4cef-8e7e-3dcd1d983bd4",
   "metadata": {},
   "source": [
    "## Data leakage is a problem in machine learning that occurs when information from the test set or future data is inadvertently used to train the model. This can lead to the model overfitting the training data and performing poorly on the test set or in production.\n",
    "\n",
    "## Examples of data leakage: Including the target variable in the features, Using features that are not stable over time, Using features that are correlated with the target variable\n",
    "\n",
    "## Data leakage can be a difficult problem to detect, but there are a few things: Use a holdout set, Use a validation set\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb1d430-8267-457d-9d94-71d6cc605ba4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5f80967-c85a-4364-8588-a74b0b126abf",
   "metadata": {},
   "source": [
    "## Q4. How can you prevent data leakage when building a machine learning model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e4969d-1831-4a02-8129-53a919066b12",
   "metadata": {},
   "source": [
    "## some ways to prevent data leakage when building a machine learning model:\n",
    "## Use a holdout set.\n",
    "## Shuffle the data.\n",
    "## Use a time-based split to separate the training and test sets.\n",
    "## Use a validation set.\n",
    "## Use a stratified split to separate the training and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71885aa5-c3da-42b9-ab1f-145dc870fb71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cbba4891-c352-4e40-81a3-1f8b5330a35b",
   "metadata": {},
   "source": [
    "## Q5. What is a confusion matrix, and what does it tell you about the performance of a classification model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2b56eb-49e9-4a57-bde9-dec085c96a88",
   "metadata": {},
   "source": [
    "## A confusion matrix is a technique for summarizing the performance of a classification algorithm.\n",
    "\n",
    "##  The confusion matrix can be used to calculate a number of metrics to assess the performance of a classification model, such as accuracy, precision, recall, and F1 score.\n",
    "## Accuracy.\n",
    "## Precision.\n",
    "## Recall.\n",
    "## F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9677b5da-0350-4872-a4d1-5cbb94423644",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1763f8a6-aa84-40f4-a64f-6ff30b09c3a0",
   "metadata": {},
   "source": [
    "##  Q6. Explain the difference between precision and recall in the context of a confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7b9519-5c57-445a-b055-0def413b4685",
   "metadata": {},
   "source": [
    "## Precision measures the accuracy of positive predictions. It is calculated by dividing the number of true positives (TP) by the sum of the true positives and false positives (FP).\n",
    "\n",
    "## Recall measures the completeness of positive predictions. It is calculated by dividing the number of true positives by the sum of the true positives and false negatives (FN).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fb01f1-86d3-4930-9c4d-1f928628db83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7089ef8-9a5c-4ec6-a985-6e6e6ca6e823",
   "metadata": {},
   "source": [
    "## Q7. How can you interpret a confusion matrix to determine which types of errors your model is making?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4584d36b-bf64-4c75-9f82-baa308b3ffd2",
   "metadata": {},
   "source": [
    "## False positives (FP): These are cases where the model predicted positive, but the actual class is negative. This can happen when the model is too lenient and classifies too many cases as positive.\n",
    "## False negatives (FN): These are cases where the model predicted negative, but the actual class is positive. This can happen when the model is too strict and classifies too many cases as negative.\n",
    "## True positives (TP): These are cases where the model predicted positive and the actual class is also positive. This is the desired outcome.\n",
    "## True negatives (TN): These are cases where the model predicted negative and the actual class is also negative. This is also the desired outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc488c2-3ee6-4e46-90ec-5f1cd1edca0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7d7d6bb-b720-4336-8cd9-02bb1482af1b",
   "metadata": {},
   "source": [
    "## Q8. What are some common metrics that can be derived from a confusion matrix, and how are they calculated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3861a930-711d-4e51-a1f1-80ff58af08e0",
   "metadata": {},
   "source": [
    "##  These are just some of the most common metrics that can be derived from a confusion matrix. The specific metrics that are used will depend on the specific application and the goals of the model.\n",
    "\n",
    "## Accuracy.\n",
    "## Precision\n",
    "## Recall\n",
    "## F1 score\n",
    "## Specificity\n",
    "## False positive rate (FPR)\n",
    "## False negative rate (FNR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7309c11-82f5-4076-aecd-d17156455f64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7250ec00-6b80-4e8f-99ad-91457dae507c",
   "metadata": {},
   "source": [
    "## Q9. What is the relationship between the accuracy of a model and the values in its confusion matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076e2a19-3304-48ed-a9dd-54c759a24644",
   "metadata": {},
   "source": [
    "##  Here are some of the most common performance measures you can use from the confusion matrix. Accuracy: It gives you the overall accuracy of the model, meaning the fraction of the total samples that were correctly classified by the classifier. To calculate accuracy, use the following formula: (TP+TN)/(TP+TN+FP+FN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9d70ad-f2dc-48f6-a5c7-2b304bc7d0af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1b77066-fec0-4566-84cf-f793c3e048f9",
   "metadata": {},
   "source": [
    "## Q10. How can you use a confusion matrix to identify potential biases or limitations in your machine learning model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f95e23-551a-4170-a348-6de664ed7d1d",
   "metadata": {},
   "source": [
    "## The number of false positives (FP): These are cases where the model predicted positive, but the actual class is negative. This can happen when the model is too lenient and classifies too many cases as positive. This can be a sign of bias if the model is more likely to make false positives for certain groups of data points, such as a certain race or gender.\n",
    "\n",
    "## The number of false negatives (FN): These are cases where the model predicted negative, but the actual class is positive. This can happen when the model is too strict and classifies too many cases as negative. This can be a sign of bias if the model is more likely to make false negatives for certain groups of data points, such as a certain race or gender.\n",
    "\n",
    "## The difference between the precision and recall: A high precision means that the model is good at avoiding false positives, while a high recall means that the model is good at avoiding false negatives. A large difference between the precision and recall can be a sign of bias.\n",
    "\n",
    "## The distribution of the data points across the classes: If the data points are not evenly distributed across the classes, then the model may be biased towards one class or the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1aa94ca-de7e-440e-8a88-3d266ade6f8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
